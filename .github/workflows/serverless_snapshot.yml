# .github/workflows/data-pipeline.yml

name: Scheduled Data Pipeline

on:
  # Runs every 15 minutes
  schedule:
    - cron: '*/15 * * * *'
  
  # Allows you to run this workflow manually from the Actions tab for testing
  workflow_dispatch:
  # push: 
  #   branches: 
  #   - "incremental_run_gha"

# This is the magic part! âœ¨
# It ensures that only one instance of this workflow runs at a time.
concurrency: 
  group: ${{ github.workflow }}
  cancel-in-progress: false

jobs:
  run-data-pipeline:
    runs-on: ubuntu-latest
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v6

      - name: install project deps 
        run: uv sync

      - name: trigger extraction
        run: uv run ticktick_fetcher.py
        env:
          TICKTICK_API_KEY: ${{ secrets.TICKTICK_API_KEY }}
          

      - name: Snapshot & dump to GCS with DuckDB ðŸ¦†
        uses: luutuankiet/dbt-action@v1.1.1
        with:
          # This pulls a docker image pre-loaded with dbt and duckdb
          dbt_command: "dbt build"
        env:
          DBT_PROFILES_DIR: .
          GCS_KEY: ${{ secrets.GCS_KEY }}
          GCS_SECRET: ${{ secrets.GCS_SECRET }}
          DBT_TARGET: "snapshot"