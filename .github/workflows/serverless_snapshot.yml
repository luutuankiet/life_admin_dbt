# .github/workflows/data-pipeline.yml

name: Scheduled Data Pipeline

on:
  # Runs every 15 minutes
  schedule:
    - cron: '*/15 * * * *'
  
  # Allows you to run this workflow manually from the Actions tab for testing
  workflow_dispatch:
    inputs:
      full_refresh:
        description: 'snapshot : Run a full refresh'
        required: false
        type: boolean
        default: false
  # push: 
  #   branches: 
  #   - "incremental_run_gha"

# This is the magic part! âœ¨
# It ensures that only one instance of this workflow runs at a time.
concurrency: 
  group: ${{ github.workflow }}
  cancel-in-progress: false

jobs:
  run-data-pipeline:
    runs-on: ubuntu-latest
    steps:
      - name: Check out repository
        uses: actions/checkout@v4
        with:
          ref: incremental_run_gha    # << this is where the 2 profiles are available     

      - name: Install uv
        uses: astral-sh/setup-uv@v6

      - name: install project deps 
        run: uv sync

      - name: trigger extraction
        run: uv run ticktick_fetcher.py
        env:
          TICKTICK_API_KEY: ${{ secrets.TICKTICK_API_KEY }}
          

      - name: Load remote state to duckdb ðŸ¦†
        uses: luutuankiet/dbt-action@v1.1.1
        with:
          # This pulls a docker image pre-loaded with dbt and duckdb
          dbt_command: "dbt build"
        env:
          DBT_PROFILES_DIR: .
          GCS_KEY: ${{ secrets.GCS_KEY }}
          GCS_SECRET: ${{ secrets.GCS_SECRET }}
          DBT_TARGET: "load_snapshot"

      - name: Snapshot and dump to GCS ðŸª£
        uses: luutuankiet/dbt-action@v1.1.1
        with:
          # This pulls a docker image pre-loaded with dbt and duckdb
          dbt_command: "dbt build ${{ github.event.inputs.full_refresh == 'true' && '--full-refresh' || '' }}"
        env:
          DBT_PROFILES_DIR: .
          GCS_KEY: ${{ secrets.GCS_KEY }}
          GCS_SECRET: ${{ secrets.GCS_SECRET }}
          DBT_TARGET: "dump_snapshot"